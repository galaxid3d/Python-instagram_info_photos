# –ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø–æ –∞–ª–∏–∞—Å—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–∫–∫–∞—É–Ω—Ç–µ Instagram, —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∏ –∏—Ö –æ–ø–∏—Å–∞–Ω–∏–µ, —Å–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–æ—Ç–æ –∏ –≤–∏–¥–µ–æ

import json
import httpx
from urllib.parse import quote
import jmespath

import os
import datetime
import requests

INDENTS = 4  # indents count in JSON and console
PAGE_PUBLICATIONS_LIMIT = 3  # how many publications need to be loaded at once
HTTP_SESSION_TIMEOUT = 300000.0  # how long does the http-session in seconds
DATETIME_FILENAME_FORMAT = '%Y-%m-%d_%H-%M-%S'  # datetime format for filename
PATH_TO_SAVE = os.path.expanduser('~') + '\\' + 'Downloads'  # path for download publications
PUBLICATION_TEXT_MIN_LEN = 10  # minimum length text of publication for saving in file

# variants of downloading
is_download_variants = {0: {'photo': False, 'video': False},
                        1: {'photo': True, 'video': False},
                        2: {'photo': False, 'video': True},
                        3: {'photo': True, 'video': True},
                        }

# web client
client = httpx.Client(
    headers={
        # this is internal ID of an Instagram backend app. It doesn't change often.
        "x-ig-app-id": "936619743392459",
        # use browser-like features
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
                      "AppleWebKit/537.36 (KHTML, like Gecko)"
                      "Chrome/62.0.3202.94 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9,ru;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept": "*/*",
    }
)


def parse_post(data: dict) -> dict:
    """Instagram publication structure"""
    result = jmespath.search("""{
        id: id,
        shortcode: shortcode,
        dimensions: dimensions,
        src: display_url,
        src_attached: edge_sidecar_to_children.edges[].node.display_url,
        has_audio: has_audio,
        video_url: video_url,
        views: video_view_count,
        plays: video_play_count,
        likes: edge_media_preview_like.count,
        location: location.name,
        datetime: taken_at_timestamp,
        related: edge_web_media_to_related_media.edges[].node.shortcode,
        type: product_type,
        video_duration: video_duration,
        music: clips_music_attribution_info,
        is_video: is_video,
        tagged_users: edge_media_to_tagged_user.edges[].node.user.username,
        captions: edge_media_to_caption.edges[].node.text,
        related_profiles: edge_related_profiles.edges[].node.username,
        comments_count: edge_media_to_parent_comment.count,
        comments_disabled: comments_disabled,
        comments_next_page: edge_media_to_parent_comment.page_info.end_cursor,
        comments: edge_media_to_parent_comment.edges[].node.{
            id: id,
            text: text,
            datetime: created_at,
            owner: owner.username,
            owner_verified: owner.is_verified,
            viewer_has_liked: viewer_has_liked,
            likes: edge_liked_by.count
        }
    }""", data)
    return result


def scrape_user_posts(user_id: str, session: httpx.Client, page_size: int = PAGE_PUBLICATIONS_LIMIT) -> None:
    """Scrape all user publications"""
    base_url = "https://www.instagram.com/graphql/query/?query_hash=e769aa130647d2354c40ea6a439bfc08&variables="
    variables = {
        "id": user_id,
        "first": page_size,
        "after": None,
    }
    while True:
        resp = session.get(base_url + quote(json.dumps(variables)))
        data = resp.json()
        posts = data["data"]["user"]["edge_owner_to_timeline_media"]
        for post in posts["edges"]:
            yield parse_post(post["node"])
        page_info = posts["page_info"]
        if not page_info["has_next_page"]:
            break
        if variables["after"] == page_info["end_cursor"]:
            break
        variables["after"] = page_info["end_cursor"]


def scrape_user(username: str) -> dict:
    """Scrape Instagram user's data"""
    result = client.get(
        f"https://i.instagram.com/api/v1/users/web_profile_info/?username={username}",
    )
    data = json.loads(result.content)
    return data["data"]["user"]


def deep_dict_get(_dict: dict, keys: list, default=None):
    """Get value from dict by list-keys"""
    for key in keys:
        if isinstance(_dict, dict):
            _dict = _dict.get(key, default)
        else:
            return default
    return _dict


def print_user_data(_dict: dict, keys: list, description: str, default=None, indents: int = INDENTS) -> None:
    """Print description and value by key in dict"""
    user_data = deep_dict_get(_dict, keys, default=default)
    if user_data and user_data not in [False, True]:
        print(f"{' ' * indents}{description}: {user_data}")
    elif user_data in [False, True]:
        result = {False: '–ù–µ—Ç', True: '–î–∞'}[user_data]
        print(f"{' ' * indents}{description}: {result}")


def print_user_information(user_data: dict) -> None:
    """Print user info"""
    print_user_data(user_data, ['id'], 'ID')
    print_user_data(user_data, ['full_name'], '–ü–æ–ª–Ω–æ–µ –∏–º—è')
    print_user_data(user_data, ['biography'], '–ë–∏–æ–≥—Ä–∞—Ñ–∏—è')
    print_user_data(user_data, ['external_url'], '–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞')
    print_user_data(user_data, ['edge_followed_by', 'count'], '–ü–æ–¥–ø–∏—Å—á–∏–∫–æ–≤')
    print_user_data(user_data, ['edge_follow', 'count'], '–ü–æ–¥–ø–∏—Å–æ–∫')
    print_user_data(user_data, ['edge_owner_to_timeline_media', 'count'], '–ü—É–±–ª–∏–∫–∞—Ü–∏–π')
    print_user_data(user_data, ['highlight_reel_count'], '–ó–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã—Ö Reels')
    print_user_data(user_data, ['is_business_account'], '–Ø–≤–ª—è–µ—Ç—Å—è –±–∏–∑–Ω–µ—Å-–∞–∫–∫–∞—É–Ω—Ç–æ–º')
    print_user_data(user_data, ['is_professional_account'], '–Ø–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º –∞–∫–∫–∞—É–Ω—Ç–æ–º')
    print_user_data(user_data, ['is_private'], '–Ø–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏–≤–∞—Ç–Ω—ã–º –∞–∫–∫–∞—É–Ω—Ç–æ–º')
    print_user_data(user_data, ['is_verified'], '–Ø–≤–ª—è–µ—Ç—Å—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–º –∞–∫–∫–∞—É–Ω—Ç–æ–º')
    print_user_data(user_data, ['profile_pic_url_hd'], '–§–æ—Ç–æ –ø—Ä–æ—Ñ–∏–ª—è')


def download_publication(filename: str, url: str, is_video: bool = False) -> None:
    """Download publication from URL to filename"""
    file_ext = 'mp4' if is_video else 'jpg'
    with requests.get(url) as publication:
        with open(f"{PATH_TO_SAVE}\{filename}.{file_ext}", 'wb') as f:
            f.write(publication.content)


if __name__ == "__main__":
    user_name = input("–í–≤–µ–¥–∏—Ç–µ –∞–ª–∏–∞—Å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ Instagram: ")
    publications_count_need = int(input("–í–≤–µ–¥–∏—Ç–µ —Å–∫–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–≤–µ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π"
                                        "\n\t(–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –≤—ã–≤–µ–¥–µ—Ç –≤—Å–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏): "))
    print("–í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ç–æ–≥–æ, –∫–∞–∫ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–∫–∞—á–∏–≤–∞—Ç—å –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:"
          "\n\t0. –ù–µ —Å–∫–∞—á–∏–≤–∞—Ç—å"
          "\n\t1. –°–∫–∞—á–∏–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ñ–æ—Ç–æ"
          "\n\t2. –°–∫–∞—á–∏–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –≤–∏–¥–µ–æ"
          "\n\t3. –°–∫–∞—á–∏–≤–∞—Ç—å –∏ —Ñ–æ—Ç–æ –∏ –≤–∏–¥–µ–æ")
    is_download = int(input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—É–±–ª–∏–∫–∞—Ü–∏–π: ").strip())
    is_download = is_download_variants[is_download]

    with httpx.Client(timeout=httpx.Timeout(HTTP_SESSION_TIMEOUT)) as session:
        # The scrape user profile to find the id and other info:
        scrape_data = scrape_user(user_name)
        user_id = scrape_data["id"]

        # Print user info
        print(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ {user_name}:")
        print_user_information(scrape_data)

        # Scrape info from user publications
        publications_count = deep_dict_get(scrape_data,
                                           ['edge_owner_to_timeline_media', 'count'])  # –í—Å–µ–≥–æ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
        publications_count_len = len(str(publications_count))
        if publications_count:
            if publications_count_need < 1:
                publications_count_need = publications_count

            # Create dir with user_name
            if is_download['photo'] or is_download['video']:
                PATH_TO_SAVE += '\\' + user_name + '\\'
                PATH_TO_SAVE.replace('\\\\', '\\')
                if not os.path.isdir(PATH_TO_SAVE):
                    os.mkdir(PATH_TO_SAVE)
                download_publication(f"_{user_name}", scrape_data['profile_pic_url_hd'])

            print("\nüëá –ü—É–±–ª–∏–∫–∞—Ü–∏–∏ –ø–æ –¥–∞—Ç–µ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è: üëá\n")

            for j, publication in enumerate(scrape_user_posts(user_id, session=session)):
                # URL of publication
                print('', end=' ' * INDENTS)
                publication_index = str(j + 1).rjust(publications_count_len)
                print(f"–ü—É–±–ª–∏–∫–∞—Ü–∏—è ‚Ññ{publication_index}: "
                      f"[https://www.instagram.com/p/{publication['shortcode']}]:")

                # Datetime of publication
                publication_datetime = datetime.datetime.fromtimestamp(publication['datetime'])
                publication_datetime_filename = publication_datetime.strftime(DATETIME_FILENAME_FORMAT)
                print(f"{' ' * INDENTS * 2}–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {publication_datetime.strftime('%Y-%m-%d %H:%M:%S')}")

                # Text of publication
                if publication['captions']:
                    print(f"{' ' * INDENTS * 2}–¢–µ–∫—Å—Ç –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:")
                    for text in publication['captions']:
                        text = text.replace('\n', '\n' + ' ' * INDENTS * 3)
                        print(f"{' ' * INDENTS * 3}{text}")
                    if is_download['photo'] or is_download['video']:
                        if len(''.join(publication['captions'])) > PUBLICATION_TEXT_MIN_LEN:
                            with open(f"{PATH_TO_SAVE}\{publication_datetime_filename}.txt", 'w', encoding='utf-8') as f:
                                for text in publication['captions']:
                                    f.write(text)

                # URL-picture
                print(f"{' ' * INDENTS * 2}–ö–∞—Ä—Ç–∏–Ω–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:")
                print(f"{' ' * INDENTS * 3}{publication['src']}")
                if is_download['photo']:
                    download_publication(publication_datetime_filename, publication['src'])

                # URL-video
                if publication['is_video']:
                    print(f"{' ' * INDENTS * 2}–í–∏–¥–µ–æ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:")
                    print(f"{' ' * INDENTS * 3}{publication['video_url']}")
                    if is_download['video']:
                        download_publication(publication_datetime_filename, publication['video_url'], is_video=True)

                # Attachments in publication
                if publication['src_attached']:
                    attachments_count_len = len(str(len(publication['src_attached']) - 1))
                    # [1:] - —Ç.–∫. –ø–µ—Ä–≤–æ–µ –≤–ª–æ–∂–µ–Ω–∏–µ —ç—Ç–æ –∏ –µ—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, –ø–æ—ç—Ç–æ–º—É –µ–≥–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    for i, attachment in enumerate(publication['src_attached'][1:]):
                        print('', end=' ' * INDENTS * 2)
                        attachment_index = str(i + 1).rjust(attachments_count_len)
                        print(f"–í–ª–æ–∂–µ–Ω–∏–µ ‚Ññ{attachment_index}: "
                              f"[https://www.instagram.com/p/{publication['shortcode']}/?img_index={i + 2}]:")
                        print(f"{' ' * INDENTS * 3}{attachment}")
                        if is_download['photo']:
                            download_publication(
                                f"{publication_datetime_filename}_{attachment_index.replace(' ', '0')}",
                                attachment)

                publications_count_need -= 1
                if not publications_count_need:
                    break
